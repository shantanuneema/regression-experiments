---
title: "Logistic, Lasso, Ridge and Elastic Net regression in R"
output: pdf_document
---

## Problem 1
```{r}

# Loading data
LR.Data <- read.table("lr.data.txt", header = T)
str(LR.Data)

```

```{r}

# Performing logistic regression
fit_logit <- glm(class ~., data = LR.Data, family = binomial(link ='logit'))
summary(fit_logit)

```

```{r}

# Plotting the model
plot(fit_logit)

```

# (a) Calculating model accuracy # 94%
```{r}

library(caret)

PClass <- fit_logit$fitted.values
PClass[PClass > 0.5 ] = 1
PClass[PClass <= 0.5 ] = 0
confusionMatrix(PClass, LR.Data$class)

```

# (b) Model coefficients # $$β_0 = -45.27, β_1 = 5.75, β_2 = 10.45$$
```{r}

fit_logit$coefficients

```

# (c) Scatter plot with decision boundary
```{r}

decisionplot <- function(model, data, class = NULL, predict_type = "class",
                         resolution = 500, ...) {
  
  if(!is.null(class)) cl <- data[,class] else cl <- 1
  data <- data[,1:2]
  k <- length(unique(cl))
  
  plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+2L,...)
  legend(6.5, 1.3, c("0", "1"), lty = c(0.5, 0.5), lwd = c(1.0, 1.2), col = c("black", "red"), pch = c(2L, 3L))

  r <- sapply(data, range, na.rm = TRUE)
  xs <- seq(r[1,1], r[2,1], length.out = resolution)
  ys <- seq(r[1,2], r[2,2], length.out = resolution)
  g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
  colnames(g) <- colnames(r)
  g <- as.data.frame(g)
  
  p <- predict(model, g, type = predict_type)
  if(is.list(p)) p <- p$class
  p <- as.factor(p)
  
  z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
  contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
          lwd = 2, levels = (1:(k-1))+.5)
}

class(fit_logit) <- c("lr", class(fit_logit))
predict.lr <- function(object, newdata, ...)
  predict.glm(object, newdata, type = "response") > .5

decisionplot(fit_logit, LR.Data, class = "class", main = "Logistic Regression")

```

# (d) Computing probabilty

The probabilty is given by:
$$P(X) = \frac {exp(β_0 + β_1x + β_2y)}{1 + exp(β_0 + β_1x + β_2y)}$$
From part (b), coefficients are: $$β_0 = -45.27, β_1 = 5.75, β_2 = 10.45$$

# (i) When x = 4, y = 1 # Probability for point (4, 1) to be classified as class 1 = 0 %. As seen in above plot (part c), it is classified as class 0
$$P(X) = \frac {exp(-45.27 + 5.75*4 + 10.45*1)}{1 + exp(-45.27 + 5.75*4 + 10.45*1)}$$
$$P(X) = \frac {7.355958e-06}{1 + 7.355958e-06} = 7.355904e^{-06} = 0.00\%$$

# (ii) When	x = 5, y = 1.2 # Probability for point (5, 1.2) to be classified as class 1 = 1.83 %. As seen in above plot (part c), it is classified class 0
$$P(X) = \frac {exp(-45.27 + 5.75*5 + 10.45*1.2)}{1 + exp(-45.27 + 5.75*5 + 10.45*1.2)}$$
$$P(X) = \frac {0.01868564}{1 + 0.01868564} = 0.01834289 = 1.83\%$$

# (iii)	When x = 6, y = 2.5 # Probability for point (6, 2.5) to be classified as class 1 = 99.99 %. As seen in above plot (part c), it is classified as class 1
$$P(X) = \frac {exp(-45.27 + 5.75*6 + 10.45*2.5)}{1 + exp(-45.27 + 5.75*6 + 10.45*2.5)}$$
$$P(X) = \frac {4662209}{1 + 4662209} = 0.9999998 = 99.99\%$$

# (iv) When x = 5, y = 2 # Probability for point (5, 2) to be classified as class 1 = 98.76 %. As seen in above plot (part c), it is classified as class 1
$$P(X) = \frac {exp(-45.27 + 5.75*5 + 10.45*2)}{1 + exp(-45.27 + 5.75*5 + 10.45*2)}$$
$$P(X) = \frac {79.83803}{1 + 79.83803} = 0.9876296 = 98.76\%$$

## Problem 2
```{r}

# Loading data
Big.Data <- read.table("big.data.txt", header = T)
head(Big.Data)

```

```{r}

# Separating data into training and testing sets
intrain <- createDataPartition(y = Big.Data$class, p = 0.80, list = FALSE)

# Train data set
traindata <- Big.Data[intrain,]

# Test data set
testdata <- Big.Data[-intrain,]

```

# (a) Removing predictors with maximum value < 2
```{r}

# Separating predictors and response
predict_var <- traindata[,-34261]
response_var <- traindata[,34261]

predictor_variables <- predict_var[, apply(predict_var, 2, function(x) max(x) > 2), drop = FALSE]

```

# (b) Performing ANOVA on each predictor
```{r}

pval <- 1
for(i in 1:(dim(predictor_variables)[2]))
{ 
  pval[i] <- anovaScores(predictor_variables[,i], response_var)
}

```

# (c)	Retaining predictors with p-values < 0.01
```{r}

PredVars <- predictor_variables[,pval < 0.01]

```

# (d) Ridge logistic regression
```{r}

set.seed(1)

library(glmnet)

lambda = 2^seq(from = -10, to = 10, by = 0.4)

cv.Ridge.fit <- cv.glmnet(as.matrix(scale(PredVars)), response_var, alpha = 0, nfolds = 5, type.measure = "class", lambda = lambda, family = "multinomial")

```

```{r}

# Plotting the model
plot(cv.Ridge.fit, main = "error vs lambda")

```

```{r}

# Plotting coefficients
plot(cv.Ridge.fit$glmnet.fit, xvar = "lambda", main = "coef vs lambda")

```


# (e) LASSO logistic regression
```{r}

set.seed(2)

lambda1 = 2^seq(from = -15, to = 1, by = 0.1)

cv.Lasso.fit <- cv.glmnet(as.matrix(scale(PredVars)), response_var, alpha = 1, nfolds = 5, type.measure = "class", lambda = lambda1, family = "multinomial")

```

```{r}

# Plotting error
plot(cv.Lasso.fit, main = "error vs lambda")

```

# (f) Elastic-net logistic regression, alpha = 0.2
```{r}

set.seed(3)

lambda2 = 2^seq(from = -15, to = 4, by = 0.1)

cv.Elastic.fit1 <- cv.glmnet(as.matrix(scale(PredVars)), response_var, alpha = 0.2, nfolds = 5, type.measure = "class", lambda = lambda2, family = "multinomial")

```

```{r}

# Plotting error
plot(cv.Elastic.fit1, main = "error vs lambda")

```

# (g) Elastic-net logistic regression, alpha = 0.8
```{r}

set.seed(4)

lambda3 = 2^seq(from = -15, to = 1, by = 0.1)

cv.Elastic.fit2 <- cv.glmnet(as.matrix(scale(PredVars)), response_var, alpha = 0.8, nfolds = 5, type.measure = "class", lambda = lambda3, family = "multinomial")

```

```{r}

# Plotting error
plot(cv.Elastic.fit2, main = "error vs lambda")

```

# Selecting predictors using LASSO model
```{r}

tmp <- coef(cv.Lasso.fit, s = "lambda.min")
T <- as.matrix(tmp[[1]])
coefs <- data.frame(T[(abs(T[,1]) > 0),])
features <- row.names(coefs)[-1]

PredVars_Lasso <- PredVars[,features]

# Combining predictors and response
PredVars_Lasso$class <- response_var

```

# (h)
```{r}

set.seed(5)

library(kernlab)

# Seting up parameters for SVM
ctrl.cross <- trainControl(method = "cv", number = 10, classProbs = TRUE, savePredictions = TRUE)
sigma = sigest(class ~ ., data = PredVars_Lasso)
gridSVMR <- expand.grid(sigma = sigma, C = 2^seq(from = -1, by = 1, to = 1))

# Tuning SVM
svm.cross <- train(class ~ ., data = PredVars_Lasso, preProc = c("center","scale"), method = 'svmRadial', metric ='Accuracy', tuneGrid = gridSVMR, trControl = ctrl.cross)

```

```{r}

# Plotting the model
plot(svm.cross)

```

```{r}

set.seed(6)

# Seting up parameters for Random forest
gridRF <- expand.grid(mtry = c(1:18))

# Tuning Random forest
RFFit.cross <- train(class ~ ., data = PredVars_Lasso, metric = "Accuracy", preProc = c("center","scale"), ntree = 200, tuneGrid = gridRF, trControl = ctrl.cross)

```

```{r}

# Plotting the model
plot(RFFit.cross)

```

# Selecting predictors using Elastic-net model, alpha = 0.2
```{r}

tmp1 <- coef(cv.Elastic.fit1, s = "lambda.min")
T1 <- as.matrix(tmp1[[1]])
coefs1 <- data.frame(T1[(abs(T1[,1]) > 0),])
features1 <- row.names(coefs1)[-1]

PredVars_E1 <- PredVars[,features1]

# Combining predictors and response
PredVars_E1$class <- response_var

```

# (i)
```{r}

set.seed(7)

# Seting up parameters for SVM
ctrl.cross1 <- trainControl(method = "cv", number = 5, classProbs = TRUE, savePredictions = TRUE)
sigma1 <- sigest(class ~ ., data = PredVars_E1)
gridSVMR1 <- expand.grid(sigma = sigma1, C = 2^seq(from = -1, by = 1, to = 2))

# Tuning SVM
svm.cross1 <- train(class ~ ., data = PredVars_E1, method = 'svmRadial', metric ='Accuracy', tuneGrid = gridSVMR1, trControl = ctrl.cross1)

```

```{r}

# Plotting the model
plot(svm.cross1)

```


```{r}

set.seed(8)

# Seting up parameters
gridRF1 <- expand.grid(mtry = c(1:5))

# Tuning Random forest
RFFit.cross1 <- train(class ~ ., data = PredVars_E1, metric = "Accuracy", ntree = 200, tuneGrid = gridRF1, trControl = ctrl.cross1)

```

```{r}

# Plotting the model
plot(RFFit.cross1)

```


# Selecting predictors using Elastic-net model, alpha = 0.8
```{r}

tmp2 <- coef(cv.Elastic.fit2, s = "lambda.min")
T2 <- as.matrix(tmp2[[1]])
coefs2 <- data.frame(T2[(abs(T2[,1]) > 0),])
features2 <- row.names(coefs2)[-1]

PredVars_E2 <- PredVars[,features2]

# Combining predictors and response
PredVars_E2$class <- response_var

```

# (j)
```{r}

set.seed(9)

# Seting up parameters for SVM
ctrl.cross2 <- trainControl(method = "cv", number = 10, classProbs = TRUE, savePredictions = TRUE)
sigma2 <- sigest(class ~ ., data = PredVars_E2)
gridSVMR2 <- expand.grid(sigma = sigma2, C = 2^seq(from = -2, by = 1, to = 2))

# Tuning SVM
svm.cross2 <- train(class ~ ., data = PredVars_E2, perProc = 'scale', method = 'svmRadial', metric ='Accuracy', tuneGrid = gridSVMR2, trControl = ctrl.cross2)

```

```{r}

# Plotting the model
plot(svm.cross2)

```

```{r}

set.seed(10)

# Seting up parameters
gridRF2 <- expand.grid(mtry = c(1:5))

# Tuning Random forest
RFFit.cross2 <- train(class ~ ., data = PredVars_E2, metric = "Accuracy", preProc = c("center","scale"), ntree = 200, tuneGrid = gridRF2, trControl = ctrl.cross2)

```

```{r}

# Plotting the model
plot(RFFit.cross2)

```

# (k) Comparing accuracies
```{r}
Pred <- predict(cv.Lasso.fit, as.matrix(PredVars), type = "class", s = cv.Lasso.fit$lambda.min)
Prob <- predict(cv.Lasso.fit, as.matrix(PredVars), type = "response", s = cv.Lasso.fit$lambda.min)
confusionMatrix(Pred, response_var)
```


# (l) Venn diagram
```{r}

library(gplots)
venn(list(LASSO = features, Elastic_0.2 = features1, Elastic_0.8 = features2))

```

